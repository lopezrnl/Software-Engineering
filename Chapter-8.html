<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Software Evolution | hungry shark evolution</title>
    <link rel="icon" type="image/png" href="Derp-Pikachu.png">
</head>

<body>
    <header>
        <h2>Software Engineering</h2>
        <nav>
            <a href="index.html">Home</a>
            <a href="chapter2.html">Software Processes</a>
            <a href="chapter8.html">Software Testing</a>
            <a href="chapter9.html">Software Evolution</a>
            <a href="chapter11.html">Dependability and Security</a>
        </nav>
        <br>
        <hr>
    </header>
    <hr>

    <main>
        <section>
            <article>
                <h3>Contents</h3>
                <a href=""><b>8.1</b> Development Testing</a><br>
                <a href=""><b>8.2</b> Test-Driven Development</a><br>
                <a href=""><b>8.3</b> Release Testing</a><br>
                <a href=""><b>8.4</b> User Testing</a>
            </article>
        </section>
        <hr>

        <section>
            <h1>Software Testing</h1>
            <p> Testing is intended to show that a program does what it is intended to do and to discover program defects before it is put into use. When you test software, you execute a program using artificial data. You check the results of the test run for errors, anomalies, or information about the program’s non-functional attributes.</p>
            <ul>
                <li> To demonstrate to the developer and the customer that the software meets its requirements. For custom software, this means that there should be at least one test for every requirement in the requirements document. For generic software products, it means that there should be tests for all of the system features, plus combinations of these features, that will be incorporated in the product release.</li>
                <li>o discover situations in which the behavior of the software is incorrect, undesirable, or does not conform to its specification. These are a consequence of software defects. Defect testing is concerned with rooting out undesirable system behavior such as system crashes, unwanted interactions with other systems, incorrect computations, and data corruption.</li>
            </ul>
        </section>

        <section>
            <h2><b>8.1</b> Development Testing</h2>
            <p> Development testing includes all testing activities that are carried out by the team developing the system. The tester of the software is usually the programmer who developed that software, although this is not always the case. Some development processes use programmer/tester pairs (Cusamano and Selby, 1998) where each  programmer has an associated tester who develops tests and assists with the testing process. For critical systems, a more formal process may be used, with a separate testing group within the development team. They are responsible for developing tests and maintaining detailed records of test results.</p>
            <p> During development, testing may be carried out at three levels of granularity:</p>
            <ol>
                <li><b>Unit testing</b>, where individual program units or object classes are tested. Unit testing should focus on testing the functionality of objects or methods.</li>
                <li><b>Component testing</b>, where several individual units are integrated to create composite components. Component testing should focus on testing component interfaces.</li>
                <li><b>System testing</b>, where some or all of the components in a system are integratedand the system is tested as a whole. System testing should focus on testing component interactions.</li>
            </ol>
            <p> Development testing is primarily a defect testing process, where the aim of testing is to discover bugs in the software. It is therefore usually interleaved with debugging—the process of locating problems with the code and changing the program to fix theseproblems.</p>
            <article>
                <h3><b>8.1.1</b> Unit Testing</h3>
                <p> Unit testing is the process of testing program components, such as methods or object classes. Individual functions or methods are the simplest type of component. Your tests should be calls to these routines with different input parameters. You can use the approaches to test case design discussed in Section 8.1.2, to design the function or method tests.</p>
                <p>When you are testing object classes, you should design your tests to provide coverage of all of the features of the object. This means that you should:</p>
                <ul>
                    <li>test all operations associated with the object;</li>
                    <li>set and check the value of all attributes associated with the object;</li>
                    <li>put the object into all possible states. This means that you should simulate all events that cause a state change.</li>
                </ul>
            </article>

            <article>
                <h3><b>8.1.2</b> Choosing Unit Test Cases</h3>
                <p> Testing is expensive and time consuming, so it is important that you choose effective unit test cases. Effectiveness, in this case, means two things:</p>
                <ol>
                    <li>The test cases should show that, when used as expected, the component that you are testing does what it is supposed to do.</li>
                    <li>If there are defects in the component, these should be revealed by test cases.</li>
                </ol>
                <p>You should therefore write two kinds of test case. The first of these should reflect normal operation of a program and should show that the component works. For example, if you are testing a component that creates and initializes a new patient record, then your test case should show that the record exists in a database and that its fields have been set as specified. The other kind of test case should be based on testing experience of where common problems arise. It should use abnormal inputs to check that these are properly processed and do not crash the component.</p>
                <p> I discuss two possible strategies here that can be effective in helping you choose test cases. These are:</p>
                <ol>
                    <li><b>Partition testing</b>, where you identify groups of inputs that have common characteristics and should be processed in the same way. You should choose tests from within each of these groups.</li>
                    <li><b>Guideline-based testing</b>, where you use testing guidelines to choose test cases. These guidelines reflect previous experience of the kinds of errors that programmers often make when developing components.</li>
                </ol>
            </article>
            
            <article>
                <h3><b>8.1.3</b> Component Testing</h3>
                <p> Software components are often composite components that are made up of several interacting objects. For example, in the weather station system, the reconfiguration component includes objects that deal with each aspect of the reconfiguration. You access the functionality of these objects through the defined component interface. Testing composite components should therefore focus on showing that the component interface behaves according to its specification. You can assume that unit tests on the individual objects within the component have been completed.</p>
                <p> There are different types of interface between program components and, consequently, different types of interface error that can occur:</p>
                <ol>
                    <li><i>Parameter interfaces</i> These are interfaces in which data or sometimes function references are passed from one component to another. Methods in an object have a parameter interface.</li>
                    <li><i>Shared memory interfaces</i> These are interfaces in which a block of memory is shared between components. Data is placed in the memory by one subsystem and retrieved from there by other sub-systems. This type of interface is often used in embedded systems, where sensors create data that is retrieved and processed by other system components.</li>
                    <li><i>Procedural interfaces</i> These are interfaces in which one component encapsulates a set of procedures that can be called by other components. Objects and reusable components have this form of interface.</li>
                    <li><i>Message passing interfaces</i> These are interfaces in which one component requests a service from another component by passing a message to it. A return message includes the results of executing the service. Some object-oriented systems have this form of interface, as do client–server systems.</li>
                </ol>
            </article>

            <article>
                <h3><b>8.1.4</b> System Testing</h3>
                <p> System testing during development involves integrating components to create a version of the system and then testing the integrated system. System testing checks that components are compatible, interact correctly and transfer the right data at the right time across their interfaces. It obviously overlaps with component testing but there are two important differences:</p>
                <ol>
                    <li>During system testing, reusable components that have been separately developed and off-the-shelf systems may be integrated with newly developed components. The complete system is then tested.</li>
                    <li>Components developed by different team members or groups may be integrated at this stage. System testing is a collective rather than an individual process. In some companies, system testing may involve a separate testing team with no involvement from designers and programmers.</li>
                </ol>
                <p>When you integrate components to create a system, you get emergent behavior. This means that some elements of system functionality only become obvious when you put the components together. This may be planned emergent behavior, which has to be tested. For example, you may integrate an authentication component with a component that updates information. You then have a system feature that restricts information updating to authorized users. Sometimes, however, the emergent behavior is unplanned and unwanted. You have to develop tests that check that the system is only doing what it is supposed to do.</p>
            </article>
        </section>

        <section>
            <h2><b>8.2</b> Test-Driven Development</h2>
            <p> Test-driven development (TDD) is an approach to program development in which you interleave testing and code development (Beck, 2002; Jeffries and Melnik, 2007). Essentially, you develop the code incrementally, along with a test for that increment. You don’t move on to the next increment until the code that you have developed passes its test. Test-driven development was introduced as part of agile methods such as Extreme Programming. However, it can also be used in plan-driven development processes.</p>
            <p>The fundamental TDD process is shown in Figure 8.9. The steps in the process are as follows:</p>
            <ol>
                <li>You start by identifying the increment of functionality that is required. This should normally be small and implementable in a few lines of code.</li>
                <li>You write a test for this functionality and implement this as an automated test. This means that the test can be executed and will report whether or not it has passed or failed.</li>
                <li>You then run the test, along with all other tests that have been implemented. Initially, you have not implemented the functionality so the new test will fail. This is deliberate as it shows that the test adds something to the test set.</li>
                <li>You then implement the functionality and re-run the test. This may involve refactoring existing code to improve it and add new code to what’s already there.</li>
                <li>Once all tests run successfully, you move on to implementing the next chunk of functionality.</li>
            </ol>
            <p>An automated testing environment, such as the JUnit environment that supports Java program testing (Massol and Husted, 2003), is essential for TDD. As the code is developed in very small increments, you have to be able to run every test each time that you add functionality or refactor the program. Therefore, the tests are embedded in a separate program that runs the tests and invokes the system that is being tested. Using this approach, it is possible to run hundreds of separate tests in a few seconds.</p>
            <p> As well as better problem understanding, other benefits of test-driven development are:</p>
            <ol>
                <li><i>Code coverage</i> In principle, every code segment that you write should have at least one associated test. Therefore, you can be confident that all of the code in the system has actually been executed. Code is tested as it is written so defects are discovered early in the development process.</li>
                <li><i>Regression testing</i> A test suite is developed incrementally as a program is developed. You can always run regression tests to check that changes to the program have not introduced new bugs.</li>
                <li><i>Simplified debugging</i> When a test fails, it should be obvious where the problem lies. The newly written code needs to be checked and modified. You do not need to use debugging tools to locate the problem. Reports of the use of test-driven development suggest that it is hardly ever necessary to use an automated debugger in test-driven development (Martin, 2007).</li>
                <li><i>System documentation</i> The tests themselves act as a form of documentation that describe what the code should be doing. Reading the tests can make it easier to understand the code.</li>
            </ol>
            <p>One of the most important benefits of test-driven development is that it reduces the costs of regression testing. Regression testing involves running test sets that have successfully executed after changes have been made to a system. The regression test checks that these changes have not introduced new bugs into the system and that the new code interacts as expected with the existing code. Regression testing is very expensive and often impractical when a system is manually tested, as the costs in time and effort are very high. In such situations, you have to try and choose the most relevant tests to re-run and it is easy to miss important tests.</p>
            <p>However, automated testing, which is fundamental to test-first development, dramatically reduces the costs of regression testing. Existing tests may be re-run quickly and cheaply. After making a change to a system in test-first development, all existing tests must run successfully before any further functionality is added. As a programmer, you can be confident that the new functionality that you have added has not caused or revealed problems with existing code.</p>
            <p>Test-driven development is of most use in new software development where the functionality is either implemented in new code or by using well-tested standard libraries. If you are reusing large code components or legacy systems then you need to write tests for these systems as a whole. Test-driven development may also be ineffective with multi-threaded systems. The different threads may be interleaved at different times in different test runs, and so may produce different results.</p>
            <p>Test-driven development has proved to be a successful approach for small and medium-sized projects. Generally, programmers who have adopted this approach are happy with it and find it a more productive way to develop software (Jeffries and Melnik, 2007). In some trials, it has been shown to lead to improved code quality; in others, the results have been inconclusive. However, there is no evidence that TDD leads to poorer quality code.</p>
        </section>

        <section>
            <h2><b>8.3</b> Release Testing</h2>
            <p> Release testing is the process of testing a particular release of a system that is intended for use outside of the development team. Normally, the system release is for customers and users. In a complex project, however, the release could be for other teams that are developing related systems. For software products, the release could be for product management who then prepare it for sale.</p>
            <p> There are two important distinctions between release testing and system testing during the development process:</p>
            <ol>
                <li>A separate team that has not been involved in the system development should be responsible for release testing.</li>
                <li>System testing by the development team should focus on discovering bugs in the system (defect testing). The objective of release testing is to check that the system meets its requirements and is good enough for external use (validation testing).</li>
            </ol>
            <p> The primary goal of the release testing process is to convince the supplier of the system that it is good enough for use. If so, it can be released as a product or delivered to the customer. Release testing, therefore, has to show that the system delivers its specified functionality, performance, and dependability, and that it does not fail during normal use. It should take into account all of the system requirements, not just the requirements of the end-users of the system.</p>
            <p>Release testing is usually a black-box testing process where tests are derived from the system specification. The system is treated as a black box whose behavior can only be determined by studying its inputs and the related outputs. Another name for this is ‘functional testing’, so-called because the tester is only concerned with functionality and not the implementation of the software.</p>  
            <article>
                <h3><b>8.3.1</b> Requirements-Based Testing</h3>
                <p>A general principle of good requirements engineering practice is that requirements should be testable; that is, the requirement should be written so that a test can be designed for that requirement. A tester can then check that the requirement has been satisfied. Requirements-based testing, therefore, is a systematic approach to test case design where you consider each requirement and derive a set of tests for it. Requirements-based testing is validation rather than defect testing—you are trying to demonstrate that the system has properly implemented its requirements.</p>
                <p>For example, consider related requirements for the MHC-PMS (introduced in Chapter 1), which are concerned with checking for drug allergies:</p>
                <p><i>If a patient is known to be allergic to any particular medication, then prescription of that medication shall result in a warning message being issued to the system user.</i></p>
                <p><i>If a prescriber chooses to ignore an allergy warning, they shall provide a reason why this has been ignored.</i></p>
                <p> To check if these requirements have been satisfied, you may need to develop several related tests:</p>
                <ol>
                    <li>Set up a patient record with no known allergies. Prescribe medication for allergies that are known to exist. Check that a warning message is not issued by the system.</li>
                    <li>Set up a patient record with a known allergy. Prescribe the medication to that the patient is allergic to, and check that the warning is issued by the system.</li>
                    <li>Set up a patient record in which allergies to two or more drugs are recorded. Prescribe both of these drugs separately and check that the correct warning for each drug is issued.</li>
                    <li>Prescribe two drugs that the patient is allergic to. Check that two warnings are correctly issued.</li>
                    <li>Prescribe a drug that issues a warning and overrule that warning. Check that the system requires the user to provide information explaining why the warning was overruled.</li>
                </ol>
                <p>You can see from this that testing a requirement does not mean just writing a single test. You normally have to write several tests to ensure that you have coverage of the requirement. You should also maintain traceability records of your requirementsbased testing, which link the tests to the specific requirements that are being tested.</p>
            </article>

            <article>
                <h3><b>8.3.2</b> Scenario Testing</h3>
                <p> Scenario testing is an approach to release testing where you devise typical scenarios of use and use these to develop test cases for the system. A scenario is a story that describes one way in which the system might be used. Scenarios should be realistic and real system users should be able to relate to them. If you have used scenarios as part of the requirements engineering process (described in Chapter 4), then you may be able to reuse these as testing scenarios.</p>
                <p>In a short paper on scenario testing, Kaner (2003) suggests that a scenario test should be a narrative story that is credible and fairly complex. It should motivate stakeholders; that is, they should relate to the scenario and believe that it is important</p>
                <p>that the system passes the test. He also suggests that it should be easy to evaluate. If there are problems with the system, then the release testing team should recognize them.</p>
                <p>It tests a number of features of the MHC-PMS:</p>
                <ol>
                    <li>Authentication by logging on to the system.</li>
                    <li>Downloading and uploading of specified patient records to a laptop.</li>
                    <li>Home visit scheduling.</li>
                    <li>Encryption and decryption of patient records on a mobile device.</li>
                    <li>Record retrieval and modification.</li>
                    <li>Links with the drugs database that maintains side-effect information.</li>
                    <li>The system for call prompting.</li>
                </ol>
                <p>When you use a scenario-based approach, you are normally testing several requirements within the same scenario. Therefore, as well as checking individual requirements, you are also checking that combinations of requirements do not cause problems.</p>
            </article>

            <article>
                <h3><b>8.3.3</b> Performance Testing</h3>
                <p> Once a system has been completely integrated, it is possible to test for emergent properties, such as performance and reliability. Performance tests have to be designed to ensure that the system can process its intended load. This usually involves running a series of tests where you increase the load until the system performance becomes unacceptable.</p>
                <p> This approach, of course, is not necessarily the best approach for defect testing. Experience has shown that an effective way to discover defects is to design tests around the limits of the system. In performance testing, this means stressing the system by making demands that are outside the design limits of the software. This is known as ‘stress testing’. For example, say you are testing a transaction processing system that is designed to process up to 300 transactions per second. You start by testing this system with fewer than 300 transactions per second. You then gradually increase the load on the system beyond 300 transactions per second until it is well beyond the maximum design load of the system and the system fails. This type of testing has two functions:</p>
                <ol>
                    <li>It tests the failure behavior of the system. Circumstances may arise through an unexpected combination of events where the load placed on the system exceeds the maximum anticipated load. In these circumstances, it is important that system failure should not cause data corruption or unexpected loss of user services. Stress testing checks that overloading the system causes it to ‘fail-soft’ rather than collapse under its load.</li>
                    <li>It stresses the system and may cause defects to come to light that would not normally be discovered. Although it can be argued that these defects are unlikely to cause system failures in normal usage, there may be unusual combinations of normal circumstances that the stress testing replicates.</li>
                </ol>
                <p> Stress testing is particularly relevant to distributed systems based on a network of processors. These systems often exhibit severe degradation when they are heavily loaded. The network becomes swamped with coordination data that the different processes must exchange. The processes become slower and slower as they wait for the required data from other processes. Stress testing helps you discover when the degradation begins so that you can add checks to the system to reject transactions beyond this point.</p>
            </article>
        </section>

        <section>
            <h2><b>8.4</b> User Testing</h2>
            <p>User or customer testing is a stage in the testing process in which users or customers provide input and advice on system testing. This may involve formally testing a system that has been commissioned from an external supplier, or could be an informal process where users experiment with a new software product to see if they like it and that it does what they need. User testing is essential, even when comprehensive system and release testing have been carried out. The reason for this is that influences from the user’s working environment have a major effect on the reliability, performance, usability, and robustness of a system.</p>
            <p>It is practically impossible for a system developer to replicate the system’s working environment, as tests in the developer’s environment are inevitably artificial. For example, a system that is intended for use in a hospital is used in a clinical environment where other things are going on, such as patient emergencies, conversations with relatives, etc. These all affect the use of a system, but developers cannot include them in their testing environment.</p>
            <p>In practice, there are three different types of user testing:</p>
            <ol>
                <li><b>Alpha testing</b>, where users of the software work with the development team to test the software at the developer’s site.</li>
                <li><b>Beta testing</b>, where a release of the software is made available to users to allow them to experiment and to raise problems that they discover with the system developers.</li>
                <li><b>Acceptance testing</b>, where customers test a system to decide whether or not it is ready to be accepted from the system developers and deployed in the customer environment.</li>
            </ol>
            <p><b>Alpha testing</b> is often used when developing software products that are sold as shrink-wrapped systems. Users of these products may be willing to get involved in the alpha testing process because this gives them early information about new system features that they can exploit. It also reduces the risk that unanticipated changes to the software will have disruptive effects on their business. However, alpha testing may also be used when custom software is being developed. Agile methods, such as XP, advocate user involvement in the development process and that users should play a key role in designing tests for the system.</p>
            <p><b>Beta testing</b> takes place when an early, sometimes unfinished, release of a software system is made available to customers and users for evaluation. Beta testers may be a selected group of customers who are early adopters of the system. Alternatively, the software may be made publicly available for use by anyone who is interested in it. Beta testing is mostly used for software products that are used in many different environments (as opposed to custom systems which are generally used in a defined environment). It is impossible for product developers to know and replicate all the environments in which the software will be used. Beta testing is therefore essential to discover interaction problems between the software and features of the environment where it is used. Beta testing is also a form of marketing— customers learn about their system and what it can do for them.</p>
            <p><b>Acceptance testing</b> is an inherent part of custom systems development. It takes place after release testing. It involves a customer formally testing a system to decide whether or not it should be accepted from the system developer. Acceptance implies that payment should be made for the system.</p>
            <p> There are six stages in the acceptance testing process. They are:</p>
            <ol>
                <li><i>Define acceptance criteria</i> This stage should, ideally, take place early in the process before the contract for the system is signed. The acceptance criteria should be part of the system contract and be agreed between the customer and the developer. In practice, however, it can be difficult to define criteria so early in the process. Detailed requirements may not be available and there may be significant requirements change during the development process.</li>
                <li><i>Plan acceptance testing</i> This involves deciding on the resources, time, and budget for acceptance testing and establishing a testing schedule. The acceptance test plan should also discuss the required coverage of the requirements and the order in which system features are tested. It should define risks to the testing process, such as system crashes and inadequate performance, and discuss how these risks can be mitigated.</li>
                <li><i>Derive acceptance tests</i> Once acceptance criteria have been established, tests have to be designed to check whether or not a system is acceptable. Acceptance tests should aim to test both the functional and non-functional characteristics (e.g., performance) of the system. They should, ideally, provide complete coverage of the system requirements. In practice, it is difficult to establish completely objective acceptance criteria. There is often scope for argument about whether or not a test shows that a criterion has definitely been met.</li>
                <li><i>Run acceptance tests</i> The agreed acceptance tests are executed on the system. Ideally, this should take place in the actual environment where the system will be used, but this may be disruptive and impractical. Therefore, a user testing environment may have to be set up to run these tests. It is difficult to automate this process as part of the acceptance tests may involve testing the interactions between end-users and the system. Some training of end-users may be required.</li>
                <li><i>Negotiate test results</i> It is very unlikely that all of the defined acceptance tests will pass and that there will be no problems with the system. If this is the case, then acceptance testing is complete and the system can be handed over. More commonly, some problems will be discovered. In such cases, the developer and the customer have to negotiate to decide if the system is good enough to be put into use. They must also agree on the developer’s response to identified problems.</li>
                <li><i>Reject/accept system</i> This stage involves a meeting between the developers and the customer to decide on whether or not the system should be accepted. If the system is not good enough for use, then further development is required to fix the identified problems. Once complete, the acceptance testing phase is repeated.</li>
            </ol>
            <p> In agile methods, such as XP, acceptance testing has a rather different meaning. In principle, it shares the notion that users should decide whether or not the system is acceptable. However, in XP, the user is part of the development team (i.e., he or she is an alpha tester) and provides the system requirements in terms of user stories. He or she is also responsible for defining the tests, which decide whether or not the developed software supports the user story. The tests are automated and development does not proceed until the story acceptance tests have passed. There is, therefore, no separate acceptance testing activity.</p>
            <p> You might think that acceptance testing is a clear-cut contractual issue. If a system does not pass its acceptance tests, then it should not be accepted and payment should not be made. However, the reality is more complex. Customers want to use the software as soon as they can because of the benefits of its immediate deployment. They may have bought new hardware, trained staff, and changed their processes. They may be willing to accept the software, irrespective of problems, because the costs of not using the software are greater than the costs of working around the problems. Therefore, the outcome of negotiations may be conditional acceptance of the system. The customer may accept the system so that deployment can begin. The system provider agrees to repair urgent problems and deliver a new version to the customer as quickly as possible.</p>
        </section>
    </main>
    <hr>

    <footer>
        <nav> 
            <a href=""><img src="icon.png" alt="Facebook"></a>
            <a href=""><img src="icon.png" alt="X"></a>
            <a href=""><img src="icon.png" alt="Instagram"></a>
            <a href=""><img src="icon.png" alt="Linkedin"></a>
            <a href=""><img src="icon.png" alt="Vimeo"></a>
            <a href=""><img src="icon.png" alt="Tiktok"></a>
        </nav>

        <section>
            <a href="">About</a>
        </section>
    </footer>
   
</body>
</html>